{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbanzai88/Book_Writing_Crew/blob/main/Cancer_KG_Pipelinev3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJUJuRE2X1Qj"
      },
      "source": [
        "# Overview"
      ],
      "id": "LJUJuRE2X1Qj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_Am4DZX1Ql"
      },
      "source": [
        "\n",
        "**Cancer KG — End‑to‑End (Colab-clean)**  \n",
        "Crawl PubMed + bioRxiv/medRxiv + ChemRxiv → Filter → Extract triples with **Ollama** → Build knowledge graph → Run category-theory inspired queries → Interactive PyVis viz with rich tooltips.\n"
      ],
      "id": "Eh_Am4DZX1Ql"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmOt8tEqX1Ql"
      },
      "source": [
        "# Settings"
      ],
      "id": "kmOt8tEqX1Ql"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e9eb06e-add2-45ed-996a-74e5141f9190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e24919-c333-4ce3-8b3b-116e9c055977"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings loaded.\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# Settings (edit these first)\n",
        "# ==============================\n",
        "\n",
        "# Search & time window\n",
        "SEARCH_TERMS = [\n",
        "    \"(cancer OR carcinoma OR neoplasm OR oncology OR tumor)\",\n",
        "    #\"(mitochondria OR mitochondrial OR metabolism OR metabolic)\"\n",
        "]\n",
        "DATE_FROM = \"2013-11-01\"          # inclusive\n",
        "DATE_TO   = None                  # None = today\n",
        "\n",
        "# Sources to include\n",
        "INCLUDE_PUBMED  = True\n",
        "INCLUDE_BIORXIV = True\n",
        "INCLUDE_MEDRXIV = True\n",
        "INCLUDE_CHEMRXIV= True\n",
        "\n",
        "# xRxiv fetch mode: number of days back (fast windowed). Example: 365 for 1 year.\n",
        "# If None/0, the xRxiv step will be skipped.\n",
        "XRXIV_WINDOW_DAYS = 365\n",
        "\n",
        "# --- Extraction settings ---\n",
        "USE_FULLTEXT = False      # << Toggle: False=abstract-only, True=try full text (opportunistic; falls back to abstract)\n",
        "MAX_PAPERS   = 1000       # safety cap for number of filtered records to process\n",
        "MAX_SENTENCES_PER_PAPER = 40  # for LLM extraction, avoid huge prompts\n",
        "\n",
        "# Ollama\n",
        "OLLAMA_MODEL = \"deepseek-r1:1.5b\"\n",
        "OLLAMA_HOST  = \"127.0.0.1\"\n",
        "OLLAMA_PORT  = 11434\n",
        "\n",
        "# Graph pruning & evidence\n",
        "MIN_EDGE_WEIGHT = 2       # drop edges with weight < this when printing/viz\n",
        "TOP_N_VIZ_NODES = 400     # keep top-N nodes by degree for viz\n",
        "\n",
        "# Files\n",
        "BASE = \"/content/kg_out\"\n",
        "RAW  = f\"{BASE}/raw\"\n",
        "FIL  = f\"{BASE}/filtered\"\n",
        "OUT  = f\"{BASE}/out\"\n",
        "\n",
        "print(\"Settings loaded.\")"
      ],
      "id": "0e9eb06e-add2-45ed-996a-74e5141f9190"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCwyhDNbX1Qo"
      },
      "source": [
        "# Install"
      ],
      "id": "rCwyhDNbX1Qo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f274722c-4a13-48f0-97ff-2cb1ee150be6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14bf79e-56c1-4b6c-c09f-e8f293e8ffd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nccl-cu12==2.21.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.23.4 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mDeps installed.\n"
          ]
        }
      ],
      "source": [
        "!pip -q install requests==2.32.3 pandas==2.2.2 networkx==3.2.1 pyvis==0.3.2 \\\n",
        "                 feedparser==6.0.11 pdfminer.six==20231228 tqdm==4.67.1 lxml\n",
        "print(\"Deps installed.\")"
      ],
      "id": "f274722c-4a13-48f0-97ff-2cb1ee150be6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc2x6Po2X1Qo"
      },
      "source": [
        "# Imports & Utils"
      ],
      "id": "Fc2x6Po2X1Qo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f7a0216-231a-46a2-ab11-32b89dbe3914",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6e8c36-f26a-4b20-b1bc-c35a94564e86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports ready. Window: 2013-11-01 → 2025-08-12\n"
          ]
        }
      ],
      "source": [
        "import os, io, re, json, gzip, time, math, textwrap, random, itertools, csv\n",
        "from pathlib import Path\n",
        "from datetime import datetime, date, timedelta\n",
        "from urllib.parse import urlencode, quote\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import requests, pandas as pd, networkx as nx\n",
        "from tqdm import tqdm\n",
        "from IPython.display import HTML, display\n",
        "from pyvis.network import Network\n",
        "from pdfminer.high_level import extract_text as pdf_extract_text\n",
        "\n",
        "BASE = Path(BASE); RAW = Path(RAW); FIL = Path(FIL); OUT = Path(OUT)\n",
        "for p in [BASE, RAW, FIL, OUT]: p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def today_iso():\n",
        "    return date.today().isoformat()\n",
        "\n",
        "if DATE_TO is None:\n",
        "    DATE_TO = today_iso()\n",
        "\n",
        "def as_iso(d):\n",
        "    if not d: return None\n",
        "    try: return datetime.fromisoformat(str(d).replace(\"Z\",\"+00:00\")).date().isoformat()\n",
        "    except:\n",
        "        s = str(d)\n",
        "        return s.split(\"T\",1)[0] if \"T\" in s else s\n",
        "\n",
        "def within_window(pubd: str, start_iso: str, end_iso: str) -> bool:\n",
        "    if not pubd: return False\n",
        "    return (pubd >= start_iso) and (pubd <= end_iso)\n",
        "\n",
        "def gz_write(path: Path, records):\n",
        "    n=0\n",
        "    with gzip.open(path, \"wt\", encoding=\"utf-8\") as f:\n",
        "        for r in records:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "            n+=1\n",
        "    return n\n",
        "\n",
        "def gz_iter(path: Path):\n",
        "    with gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            try: yield json.loads(line)\n",
        "            except: continue\n",
        "\n",
        "def clean_text(s: str, max_len=None):\n",
        "    s = (s or \"\").replace(\"\\x00\",\"\").strip()\n",
        "    if max_len and len(s) > max_len:\n",
        "        s = s[:max_len] + \" …\"\n",
        "    return s\n",
        "\n",
        "print(\"Imports ready. Window:\", DATE_FROM, \"→\", DATE_TO)"
      ],
      "id": "2f7a0216-231a-46a2-ab11-32b89dbe3914"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqAEZ1MEX1Qq"
      },
      "source": [
        "# Ollama"
      ],
      "id": "pqAEZ1MEX1Qq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ee5880b-961f-446a-af4a-5cc1d6b215bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbaa35e-e1a3-4d72-daa6-e498c4fd6794"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up Ollama…\n",
            "Ollama API: OK\n",
            "Pulling model: deepseek-r1:1.5b\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "import subprocess, socket, threading\n",
        "\n",
        "OLLAMA_BASE = f\"http://{OLLAMA_HOST}:{OLLAMA_PORT}\"\n",
        "\n",
        "def port_open(host, port):\n",
        "    try:\n",
        "        with socket.create_connection((host, port), timeout=1):\n",
        "            return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def start_ollama():\n",
        "    if port_open(OLLAMA_HOST, OLLAMA_PORT):\n",
        "        print(\"Ollama already running at\", OLLAMA_BASE)\n",
        "        return\n",
        "    print(\"Setting up Ollama…\")\n",
        "    # install ollama\n",
        "    !curl -fsSL https://ollama.com/install.sh -o install.sh\n",
        "    !bash install.sh >/dev/null 2>&1 || true\n",
        "    # serve in background\n",
        "    def _serve():\n",
        "        subprocess.Popen([\"ollama\",\"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    threading.Thread(target=_serve, daemon=True).start()\n",
        "    time.sleep(6)\n",
        "    print(\"Ollama API:\", \"OK\" if port_open(OLLAMA_HOST, OLLAMA_PORT) else \"FAILED\")\n",
        "\n",
        "def pull_model(model):\n",
        "    try:\n",
        "        print(\"Pulling model:\", model)\n",
        "        !ollama pull {model}\n",
        "    except Exception as e:\n",
        "        print(\"Model pull error (continuing):\", e)\n",
        "\n",
        "start_ollama()\n",
        "pull_model(OLLAMA_MODEL)"
      ],
      "id": "0ee5880b-961f-446a-af4a-5cc1d6b215bb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQG4SDkCX1Qr"
      },
      "source": [
        "# Crawl: PubMed"
      ],
      "id": "PQG4SDkCX1Qr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c218694f-e2c6-485c-9ea9-0a55311eaacd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fee311d-a85c-43e6-b377-d3e0f3ee17d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling PubMed for 2013-11-01 → 2025-08-12\n",
            "  PubMed ids for [(cancer OR carcinoma OR neoplasm OR oncology OR tumor)]: 9999\n",
            "Wrote PubMed: 9999\n"
          ]
        }
      ],
      "source": [
        "S = requests.Session()\n",
        "S.headers.update({\"User-Agent\":\"colab-cancer-kg/0.1\", \"Accept\":\"application/json\"})\n",
        "BASE_EUTIL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
        "\n",
        "def pubmed_search(term, dfrom, dto, retmax=100000):\n",
        "    query = f'({term}) AND (\"{dfrom}\"[Date - Publication] : \"{dto}\"[Date - Publication])'\n",
        "    params = {\"db\":\"pubmed\",\"term\":query,\"retmode\":\"json\",\"retmax\":retmax}\n",
        "    r = S.get(f\"{BASE_EUTIL}/esearch.fcgi\", params=params, timeout=60)\n",
        "    try:\n",
        "        data = r.json()\n",
        "        return data.get(\"esearchresult\",{}).get(\"idlist\", [])\n",
        "    except Exception:\n",
        "        # Retry XML fallback\n",
        "        ids = re.findall(r\"<Id>(\\d+)</Id>\", r.text)\n",
        "        return ids\n",
        "\n",
        "def pubmed_fetch(pmids):\n",
        "    out=[]\n",
        "    BATCH=200\n",
        "    for i in range(0, len(pmids), BATCH):\n",
        "        ids = pmids[i:i+BATCH]\n",
        "        params = {\"db\":\"pubmed\",\"id\":\",\".join(ids),\"retmode\":\"json\"}\n",
        "        r = S.get(f\"{BASE_EUTIL}/esummary.fcgi\", params=params, timeout=60)\n",
        "        if not r.ok:\n",
        "            continue\n",
        "        try:\n",
        "            data = r.json()[\"result\"]\n",
        "        except Exception:\n",
        "            continue\n",
        "        for pid in ids:\n",
        "            rec = data.get(pid)\n",
        "            if not rec: continue\n",
        "            out.append({\n",
        "                \"server\": \"pubmed\",\n",
        "                \"pmid\": pid,\n",
        "                \"title\": rec.get(\"title\"),\n",
        "                \"abstract\": None,\n",
        "                \"doi\": (rec.get(\"elocationid\") or \"\").replace(\"doi:\",\"\").strip() if rec.get(\"elocationid\") else None,\n",
        "                \"published_date\": as_iso(rec.get(\"pubdate\")),\n",
        "                \"url\": f\"https://pubmed.ncbi.nlm.nih.gov/{pid}/\"\n",
        "            })\n",
        "    # abstracts\n",
        "    BATCH=200\n",
        "    for i in range(0, len(pmids), BATCH):\n",
        "        ids = pmids[i:i+BATCH]\n",
        "        params = {\"db\":\"pubmed\",\"id\":\",\".join(ids),\"retmode\":\"xml\"}\n",
        "        r = S.get(f\"{BASE_EUTIL}/efetch.fcgi\", params=params, timeout=60)\n",
        "        if not r.ok: continue\n",
        "        r.encoding=\"utf-8\"; txt=r.text\n",
        "        for pid in ids:\n",
        "            m = re.search(rf\"<PubmedArticle>[\\s\\S]*?<PMID[^>]*?>{pid}</PMID>[\\s\\S]*?</PubmedArticle>\", txt)\n",
        "            if not m: continue\n",
        "            block = m.group(0)\n",
        "            ab = \" \".join(re.findall(r\"<AbstractText[^>]*>(.*?)</AbstractText>\", block, flags=re.S)) or None\n",
        "            for rec in out:\n",
        "                if rec[\"pmid\"] == pid:\n",
        "                    rec[\"abstract\"] = ab\n",
        "                    break\n",
        "    return out\n",
        "\n",
        "if INCLUDE_PUBMED:\n",
        "    print(\"Crawling PubMed for\", DATE_FROM, \"→\", DATE_TO)\n",
        "    pm_all=[]\n",
        "    for term in SEARCH_TERMS:\n",
        "        ids = pubmed_search(term, DATE_FROM, DATE_TO)\n",
        "        print(f\"  PubMed ids for [{term}]:\", len(ids))\n",
        "        pm_all.extend(ids)\n",
        "    pm_all = list(dict.fromkeys(pm_all))\n",
        "    pubmed_records = pubmed_fetch(pm_all)\n",
        "    n = gz_write(Path(RAW) / \"pubmed.jsonl.gz\", pubmed_records)\n",
        "    print(\"Wrote PubMed:\", n)\n",
        "else:\n",
        "    print(\"Skipping PubMed.\")"
      ],
      "id": "c218694f-e2c6-485c-9ea9-0a55311eaacd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq0ldYd1X1Qs"
      },
      "source": [
        "# Crawl: bioRxiv/medRxiv & ChemRxiv (windowed)"
      ],
      "id": "Sq0ldYd1X1Qs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "711d40a1-402c-4073-b882-bebb7ce8a4d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c84c0e8-bab9-4351-a90d-42b4fb6f02b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xRxiv window: 2024-08-12 → 2025-08-12\n",
            "  bioRxiv: 100\n",
            "  medRxiv: 100\n",
            "  ChemRxiv: 0\n"
          ]
        }
      ],
      "source": [
        "sess = requests.Session()\n",
        "sess.headers.update({\"Accept\":\"application/json\",\"User-Agent\":\"colab-cancer-kg/0.1\"})\n",
        "sess.trust_env = False\n",
        "\n",
        "def biorxiv_fetch(server, dfrom, dto):\n",
        "    base = f\"https://api.biorxiv.org/details/{server}/{dfrom}/{dto}\"\n",
        "    cursor = 0; seen=set()\n",
        "    while True:\n",
        "        r = sess.get(f\"{base}/{cursor}\", timeout=60)\n",
        "        if not r.ok: break\n",
        "        try:\n",
        "            data = r.json()\n",
        "        except Exception:\n",
        "            break\n",
        "        coll = data.get(\"collection\",[])\n",
        "        if not coll: break\n",
        "        for rec in coll:\n",
        "            yield {\n",
        "                \"server\": server,\n",
        "                \"id\": rec.get(\"doi\"),\n",
        "                \"title\": rec.get(\"title\"),\n",
        "                \"abstract\": rec.get(\"abstract\"),\n",
        "                \"doi\": rec.get(\"doi\"),\n",
        "                \"published_date\": as_iso(rec.get(\"date\")),\n",
        "                \"url\": f\"https://www.{server}.org/content/{rec.get('doi','').split('/')[-1]}\"\n",
        "            }\n",
        "        nxt = data.get(\"next_cursor\")\n",
        "        if not nxt or nxt in seen: break\n",
        "        seen.add(nxt); cursor=nxt; time.sleep(0.15)\n",
        "\n",
        "def chemrxiv_fetch(dfrom, dto, page_size=200, max_pages=60):\n",
        "    base = \"https://api.figshare.com/v2/articles/search\"\n",
        "    page=1; total=0\n",
        "    while page<=max_pages:\n",
        "        params = {\"search_for\":\"ChemRxiv\", \"page\":page, \"page_size\":page_size,\n",
        "                  \"published_since\": dfrom, \"published_until\": dto,\n",
        "                  \"order\":\"published_date\", \"order_direction\":\"desc\"}\n",
        "        r = sess.get(base, params=params, timeout=60)\n",
        "        if not r.ok: break\n",
        "        try:\n",
        "            items = r.json()\n",
        "        except Exception:\n",
        "            break\n",
        "        if not items: break\n",
        "        for it in items:\n",
        "            pubd = as_iso(it.get(\"published_date\") or (it.get(\"timeline\") or {}).get(\"published\"))\n",
        "            yield {\n",
        "                \"server\":\"chemrxiv\",\n",
        "                \"id\": it.get(\"id\"),\n",
        "                \"title\": it.get(\"title\"),\n",
        "                \"abstract\": it.get(\"description\"),\n",
        "                \"doi\": it.get(\"doi\"),\n",
        "                \"published_date\": pubd,\n",
        "                \"url\": it.get(\"url_public_html\") or it.get(\"url\")\n",
        "            }\n",
        "            total+=1\n",
        "        page+=1; time.sleep(0.12)\n",
        "\n",
        "if any([INCLUDE_BIORXIV, INCLUDE_MEDRXIV, INCLUDE_CHEMRXIV]) and XRXIV_WINDOW_DAYS:\n",
        "    dfrom = (datetime.fromisoformat(DATE_TO).date() - timedelta(days=int(XRXIV_WINDOW_DAYS))).isoformat()\n",
        "    dto   = DATE_TO\n",
        "    print(\"xRxiv window:\", dfrom, \"→\", dto)\n",
        "    if INCLUDE_BIORXIV:\n",
        "        bx = list(biorxiv_fetch(\"biorxiv\", dfrom, dto)); print(\"  bioRxiv:\", len(bx))\n",
        "        gz_write(Path(RAW)/\"biorxiv.jsonl.gz\", bx)\n",
        "    if INCLUDE_MEDRXIV:\n",
        "        mx = list(biorxiv_fetch(\"medrxiv\", dfrom, dto)); print(\"  medRxiv:\", len(mx))\n",
        "        gz_write(Path(RAW)/\"medrxiv.jsonl.gz\", mx)\n",
        "    if INCLUDE_CHEMRXIV:\n",
        "        cx = list(chemrxiv_fetch(dfrom, dto)); print(\"  ChemRxiv:\", len(cx))\n",
        "        gz_write(Path(RAW)/\"chemrxiv.jsonl.gz\", cx)\n",
        "else:\n",
        "    print(\"Skipping xRxiv windowed crawl (set XRXIV_WINDOW_DAYS).\")"
      ],
      "id": "711d40a1-402c-4073-b882-bebb7ce8a4d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oNAIjZvX1Qs"
      },
      "source": [
        "# Filter & De‑dupe"
      ],
      "id": "6oNAIjZvX1Qs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9458405-e504-4c8c-b272-2edf1a4187aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7092a973-ce94-4bfb-835b-b70deb8f3659"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered kept: 10021 → /content/kg_out/filtered/filtered_all.jsonl.gz\n"
          ]
        }
      ],
      "source": [
        "KEYWORDS = set([kw.lower() for kw in [\n",
        "    \"cancer\",\"neoplasm\",\"carcinoma\",\"oncology\",\"tumor\",\"tumour\",\n",
        "    \"mitochondria\",\"mitochondrial\",\"metabolism\",\"metabolic\"\n",
        "]])\n",
        "\n",
        "def record_passes(rec):\n",
        "    title = (rec.get(\"title\") or \"\").lower()\n",
        "    ab    = (rec.get(\"abstract\") or \"\").lower()\n",
        "    return any(k in title or k in ab for k in KEYWORDS)\n",
        "\n",
        "def normalize_rec(rec):\n",
        "    out = dict(rec)\n",
        "    out[\"published_date\"] = as_iso(out.get(\"published_date\") or out.get(\"date\"))\n",
        "    return out\n",
        "\n",
        "paths = [Path(RAW)/\"pubmed.jsonl.gz\", Path(RAW)/\"biorxiv.jsonl.gz\", Path(RAW)/\"medrxiv.jsonl.gz\", Path(RAW)/\"chemrxiv.jsonl.gz\"]\n",
        "seen=set(); filtered=[]\n",
        "for p in paths:\n",
        "    if not p.exists(): continue\n",
        "    for rec in gz_iter(p):\n",
        "        r = normalize_rec(rec)\n",
        "        if not within_window(r.get(\"published_date\") or \"1900-01-01\", DATE_FROM, DATE_TO):\n",
        "            continue\n",
        "        if not record_passes(r):\n",
        "            continue\n",
        "        k = ((r.get(\"doi\") or \"\").lower().strip(),\n",
        "             (r.get(\"pmid\") or r.get(\"id\") or r.get(\"url\") or \"\").lower().strip(),\n",
        "             r.get(\"published_date\") or \"\")\n",
        "        if k in seen:\n",
        "            continue\n",
        "        seen.add(k); filtered.append(r)\n",
        "\n",
        "FIL_COMBINED = Path(FIL)/\"filtered_all.jsonl.gz\"\n",
        "n = gz_write(FIL_COMBINED, filtered)\n",
        "print(f\"Filtered kept: {n} → {FIL_COMBINED}\")"
      ],
      "id": "c9458405-e504-4c8c-b272-2edf1a4187aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B6luDZhX1Qt"
      },
      "source": [
        "# Triples (Ollama)"
      ],
      "id": "0B6luDZhX1Qt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d617e57-8a6c-4fec-9817-82a70d8f3c24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0487c5d3-4cf6-494e-e4d2-37f4f75551c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2it [54:30, 1716.93s/it]"
          ]
        }
      ],
      "source": [
        "API = f\"http://{OLLAMA_HOST}:{OLLAMA_PORT}/api/generate\"\n",
        "\n",
        "def split_sentences(text: str):\n",
        "    text = re.sub(r\"\\s+\", \" \", text or \"\").strip()\n",
        "    return re.split(r\"(?<=[\\.\\?\\!])\\s+(?=[A-Z(])\", text)\n",
        "\n",
        "def pdf_text_from_url(url: str, timeout=45) -> str:\n",
        "    try:\n",
        "        r = requests.get(url, timeout=timeout)\n",
        "        r.raise_for_status()\n",
        "        if r.headers.get(\"content-type\",\"\").lower().startswith(\"application/pdf\"):\n",
        "            return pdf_extract_text(io.BytesIO(r.content)) or \"\"\n",
        "        return \"\"\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def try_fulltext(rec):\n",
        "    pdf_url = rec.get(\"pdf_url\")\n",
        "    if not pdf_url and rec.get(\"url\",\"\").endswith(\".pdf\"):\n",
        "        pdf_url = rec.get(\"url\")\n",
        "    if pdf_url:\n",
        "        txt = pdf_text_from_url(pdf_url)\n",
        "        if len(txt) > 500:\n",
        "            return txt, \"fulltext(pdf)\"\n",
        "    return clean_text(rec.get(\"title\",\"\") + \"\\n\\n\" + (rec.get(\"abstract\") or rec.get(\"description\") or \"\")), \"abstract\"\n",
        "\n",
        "TRIPLES_OUT = Path(OUT)/\"triples.jsonl.gz\"\n",
        "\n",
        "def llm_extract_triples(sent):\n",
        "    prompt = f\"\"\"Extract biomedical mechanism triples as JSON array with objects:\n",
        "{{\n",
        "  \"subject\": \"...\",\n",
        "  \"relation\": \"...\",\n",
        "  \"object\": \"...\"\n",
        "}}\n",
        "Subjects/objects should be short phrases already present in the sentence; keep biomedical nouns or noun phrases.\n",
        "Use simple causal/association relations (e.g., \"activates\",\"inhibits\",\"causes\",\"associated_with\",\"induces\",\"increases\",\"decreases\").\n",
        "Return ONLY JSON.\n",
        "\n",
        "Sentence: {json.dumps(sent)}\n",
        "\"\"\"\n",
        "    try:\n",
        "        r = requests.post(API, json={\"model\": OLLAMA_MODEL, \"prompt\": prompt, \"stream\": False, \"options\":{\"temperature\":0.0}}, timeout=120)\n",
        "        r.raise_for_status()\n",
        "        raw = r.json().get(\"response\",\"\").strip()\n",
        "        m = re.search(r\"\\[[\\s\\S]*\\]\", raw)\n",
        "        if not m:\n",
        "            return []\n",
        "        data = json.loads(m.group(0))\n",
        "        out=[]\n",
        "        for d in data:\n",
        "            s = clean_text(d.get(\"subject\",\"\"))\n",
        "            p = clean_text(d.get(\"relation\",\"\"))\n",
        "            o = clean_text(d.get(\"object\",\"\"))\n",
        "            if s and p and o and s.lower()!=o.lower():\n",
        "                out.append((s,p,o))\n",
        "        return out\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def extract_triples(inputs_gz: Path, max_papers=MAX_PAPERS, max_sents=MAX_SENTENCES_PER_PAPER):\n",
        "    out_f = gzip.open(TRIPLES_OUT, \"wt\", encoding=\"utf-8\")\n",
        "    n=0\n",
        "    for i, rec in enumerate(tqdm(gz_iter(inputs_gz), total=None)):\n",
        "        if max_papers and i>=max_papers: break\n",
        "        text, src = try_fulltext(rec) if USE_FULLTEXT else (clean_text(rec.get(\"title\",\"\")+\"\\n\\n\"+(rec.get(\"abstract\") or rec.get(\"description\") or \"\")), \"abstract\")\n",
        "        sents = split_sentences(text)[:max_sents]\n",
        "        paper_id = rec.get(\"pmid\") or rec.get(\"doi\") or rec.get(\"id\") or rec.get(\"url\") or f\"rec_{i}\"\n",
        "        for sent in sents:\n",
        "            triples = llm_extract_triples(sent)\n",
        "            for (s,p,o) in triples:\n",
        "                out_f.write(json.dumps({\n",
        "                    \"paper_id\": paper_id, \"source\": rec.get(\"server\"),\n",
        "                    \"subject\": s, \"predicate\": p, \"object\": o,\n",
        "                    \"sentence\": sent\n",
        "                }, ensure_ascii=False) + \"\\n\")\n",
        "                n+=1\n",
        "    out_f.close()\n",
        "    print(\"Triples →\", TRIPLES_OUT, f\"(rows: {n})\")\n",
        "\n",
        "extract_triples(FIL_COMBINED)"
      ],
      "id": "7d617e57-8a6c-4fec-9817-82a70d8f3c24"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtpBFubnX1Qt"
      },
      "source": [
        "# Build KG & Prune"
      ],
      "id": "QtpBFubnX1Qt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5cba206-26ec-437a-983e-56872bffe45a"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def canon(x):\n",
        "    return re.sub(r\"\\s+\",\" \", (x or \"\").strip()).lower()\n",
        "\n",
        "G = nx.DiGraph()\n",
        "edge_examples = defaultdict(list)\n",
        "\n",
        "for rec in gz_iter(Path(OUT)/\"triples.jsonl.gz\"):\n",
        "    s = canon(rec.get(\"subject\"))\n",
        "    o = canon(rec.get(\"object\"))\n",
        "    p = canon(rec.get(\"predicate\"))\n",
        "    if not s or not o or not p or s==o:\n",
        "        continue\n",
        "    G.add_node(s, label=rec.get(\"subject\"))\n",
        "    G.add_node(o, label=rec.get(\"object\"))\n",
        "    if G.has_edge(s,o):\n",
        "        G[s][o][\"weight\"] = G[s][o].get(\"weight\",0) + 1\n",
        "        G[s][o].setdefault(\"relations\", Counter())\n",
        "        G[s][o][\"relations\"][p]+=1\n",
        "    else:\n",
        "        G.add_edge(s,o, weight=1, relations=Counter({p:1}))\n",
        "    if len(edge_examples[(s,o)])<3:\n",
        "        edge_examples[(s,o)].append({\n",
        "            \"predicate\": rec.get(\"predicate\"),\n",
        "            \"sentence\": rec.get(\"sentence\"),\n",
        "            \"paper_id\": rec.get(\"paper_id\"),\n",
        "            \"source\": rec.get(\"source\")\n",
        "        })\n",
        "\n",
        "print(f\"Graph so far: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "to_drop = [(u,v) for u,v,d in G.edges(data=True) if d.get(\"weight\",1) < MIN_EDGE_WEIGHT]\n",
        "G.remove_edges_from(to_drop)\n",
        "iso = [n for n in list(G.nodes) if G.degree(n)==0]\n",
        "G.remove_nodes_from(iso)\n",
        "print(f\"After pruning (weight≥{MIN_EDGE_WEIGHT}): {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "\n",
        "if G.number_of_nodes() > TOP_N_VIZ_NODES:\n",
        "    degs = dict(G.degree(weight=\"weight\"))\n",
        "    keep = set(sorted(degs, key=degs.get, reverse=True)[:TOP_N_VIZ_NODES])\n",
        "    G_viz = G.subgraph(keep).copy()\n",
        "else:\n",
        "    G_viz = G.copy()\n",
        "\n",
        "EDGE_EXAMPLES_PATH = Path(OUT)/\"edge_examples.jsonl.gz\"\n",
        "with gzip.open(EDGE_EXAMPLES_PATH, \"wt\", encoding=\"utf-8\") as f:\n",
        "    for (u,v), exs in edge_examples.items():\n",
        "        f.write(json.dumps({\"u\":u,\"v\":v,\"examples\":exs}, ensure_ascii=False)+\"\\n\")\n",
        "print(\"Saved edge examples →\", EDGE_EXAMPLES_PATH)"
      ],
      "id": "a5cba206-26ec-437a-983e-56872bffe45a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4jWp3ZUX1Qu"
      },
      "source": [
        "# Category‑Theory Queries + Evidence"
      ],
      "id": "-4jWp3ZUX1Qu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cf5088f-c768-4fa2-aee2-913be8a3640a"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def shared_precursors(G, outcome1, outcome2, max_depth=2):\n",
        "    if outcome1 not in G or outcome2 not in G:\n",
        "        return set()\n",
        "    preds1 = {p for p in nx.ancestors(G, outcome1) if nx.shortest_path_length(G, p, outcome1) <= max_depth}\n",
        "    preds2 = {p for p in nx.ancestors(G, outcome2) if nx.shortest_path_length(G, p, outcome2) <= max_depth}\n",
        "    return preds1 & preds2\n",
        "\n",
        "def max_product_paths(G, cutoff=4):\n",
        "    strength = {}\n",
        "    for src in G.nodes:\n",
        "        for tgt in G.nodes:\n",
        "            if src==tgt: continue\n",
        "            if not nx.has_path(G, src, tgt): continue\n",
        "            best = 0.0\n",
        "            for path in nx.all_simple_paths(G, src, tgt, cutoff=cutoff):\n",
        "                w=1.0\n",
        "                ok=True\n",
        "                for u,v in zip(path, path[1:]):\n",
        "                    ew = G[u][v].get(\"weight\",0)\n",
        "                    if ew < MIN_EDGE_WEIGHT: ok=False; break\n",
        "                    w *= ew\n",
        "                if ok and w>best:\n",
        "                    best=w\n",
        "            if best>0: strength[(src,tgt)] = best\n",
        "    return strength\n",
        "\n",
        "def find_contradictions(G):\n",
        "    contradictions=[]\n",
        "    for a in G.nodes:\n",
        "        for b in G.successors(a):\n",
        "            for c in G.successors(a):\n",
        "                if b==c: continue\n",
        "                common = set(G.successors(b)) & set(G.successors(c))\n",
        "                for d in common:\n",
        "                    r1 = next(iter(G[a][b].get(\"relations\",{})), \"\")\n",
        "                    r2 = next(iter(G[b][d].get(\"relations\",{})), \"\")\n",
        "                    r3 = next(iter(G[a][c].get(\"relations\",{})), \"\")\n",
        "                    r4 = next(iter(G[c][d].get(\"relations\",{})), \"\")\n",
        "                    if (r1,r2) != (r3,r4):\n",
        "                        contradictions.append((a,b,c,d,(r1,r2),(r3,r4)))\n",
        "    return contradictions\n",
        "\n",
        "def edge_examples_lookup(u,v, max_n=3):\n",
        "    return ex_index.get((u,v), [])[:max_n]\n",
        "\n",
        "ex_index=defaultdict(list)\n",
        "for row in gz_iter(Path(OUT)/\"edge_examples.jsonl.gz\"):\n",
        "    ex_index[(row[\"u\"],row[\"v\"])] = row[\"examples\"]\n",
        "\n",
        "OUTCOME1, OUTCOME2 = \"p53 pathway\", \"apoptosis\"\n",
        "def canon(x):\n",
        "    return re.sub(r\"\\s+\",\" \", (x or \"\").strip()).lower()\n",
        "o1, o2 = canon(OUTCOME1), canon(OUTCOME2)\n",
        "\n",
        "print(f\"Shared precursors for [{OUTCOME1}] & [{OUTCOME2}] (weight≥{MIN_EDGE_WEIGHT}):\")\n",
        "shared = [n for n in shared_precursors(G, o1, o2) if G.degree(n)>0]\n",
        "print(\" count:\", len(shared))\n",
        "print(\" examples:\", [G.nodes[s].get(\"label\", s) for s in list(shared)[:5]])\n",
        "\n",
        "print(\"\\nShared precursors for [mitochondrial dysfunction] & [apoptosis]:\")\n",
        "o3 = canon(\"mitochondrial dysfunction\")\n",
        "shared2 = [n for n in shared_precursors(G, o3, o2) if G.degree(n)>0]\n",
        "print(\" count:\", len(shared2))\n",
        "print(\" examples:\", [G.nodes[s].get(\"label\", s) for s in list(shared2)[:5]])\n",
        "\n",
        "strengths = max_product_paths(G, cutoff=4)\n",
        "top = sorted(strengths.items(), key=lambda x: -x[1])[:10]\n",
        "print(\"\\nTop max-product links (up to 10):\")\n",
        "for (src, tgt), val in top:\n",
        "    print(f\"  {G.nodes[src].get('label',src)} → {G.nodes[tgt].get('label',tgt)} : {val:.4f}\")\n",
        "\n",
        "contr = find_contradictions(G)\n",
        "print(\"Potential contradictions:\", len(contr))\n",
        "for row in contr[:5]:\n",
        "    a,b,c,d,eff1,eff2 = row\n",
        "    print(\"\\nSquare:\", G.nodes[a].get(\"label\",a), \"→\", G.nodes[b].get(\"label\",b), \"→\", G.nodes[d].get(\"label\",d),\n",
        "          \" vs \",\n",
        "          G.nodes[a].get(\"label\",a), \"→\", G.nodes[c].get(\"label\",c), \"→\", G.nodes[d].get(\"label\",d))\n",
        "    print(\"  effects:\", eff1, \"vs\", eff2)\n",
        "    ex_ab = edge_examples_lookup(a,b,2)\n",
        "    ex_ac = edge_examples_lookup(a,c,2)\n",
        "    print(\"  ex a→b:\", [e[\"sentence\"] for e in ex_ab])\n",
        "    print(\"  ex a→c:\", [e[\"sentence\"] for e in ex_ac])"
      ],
      "id": "8cf5088f-c768-4fa2-aee2-913be8a3640a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BelKjAeX1Qu"
      },
      "source": [
        "# Interactive KG (PyVis)"
      ],
      "id": "1BelKjAeX1Qu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29c223b5-9071-42e5-bd78-f0fb528a4875"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def color_for(label):\n",
        "    L = label.lower()\n",
        "    if any(k in L for k in [\"apoptosis\",\"death\",\"proliferat\",\"survival\",\"growth\"]):\n",
        "        return \"#7E57C2\"\n",
        "    if any(k in L for k in [\"mitochond\",\"metab\",\"ros\",\"oxid\"]):\n",
        "        return \"#43A047\"\n",
        "    if any(k in L for k in [\"cancer\",\"tumor\",\"tumour\",\"carcinoma\",\"neoplasm\",\"metast\"]):\n",
        "        return \"#E53935\"\n",
        "    if any(k in L for k in [\"p53\",\"tp53\",\"dna\",\"damage\",\"repair\",\"pathway\"]):\n",
        "        return \"#1E88E5\"\n",
        "    return \"#9AA0A6\"\n",
        "\n",
        "net = Network(height=\"780px\", width=\"100%\", directed=True, notebook=True, cdn_resources=\"in_line\")\n",
        "net.barnes_hut(gravity=-20000, central_gravity=0.15, spring_length=160, spring_strength=0.01, damping=0.86)\n",
        "\n",
        "deg = dict(G_viz.degree(weight=\"weight\"))\n",
        "in_deg  = dict(G_viz.in_degree(weight=\"weight\"))\n",
        "out_deg = dict(G_viz.out_degree(weight=\"weight\"))\n",
        "\n",
        "def size_fn(x):\n",
        "    import math\n",
        "    return max(8, 10 + 2*math.log1p(x))\n",
        "\n",
        "for n, data in G_viz.nodes(data=True):\n",
        "    label = data.get(\"label\", n)\n",
        "    w = deg.get(n,1)\n",
        "    node_size = size_fn(w)\n",
        "    color = color_for(label)\n",
        "    title = f\"<b>{label}</b><br/>deg(w)={w}\"\n",
        "    net.add_node(n, label=label, title=title, value=node_size, color=color, shape=\"dot\")\n",
        "\n",
        "def clean_text(s, max_len=None):\n",
        "    s = (s or \"\").replace(\"\\x00\",\"\").strip()\n",
        "    if max_len and len(s) > max_len:\n",
        "        s = s[:max_len] + \" …\"\n",
        "    return s\n",
        "\n",
        "# Load examples\n",
        "ex_index = {}\n",
        "for row in gz_iter(Path(OUT)/\"edge_examples.jsonl.gz\"):\n",
        "    ex_index[(row[\"u\"],row[\"v\"])] = row[\"examples\"]\n",
        "\n",
        "for u, v, d in G_viz.edges(data=True):\n",
        "    weight = int(d.get(\"weight\",1))\n",
        "    rels   = d.get(\"relations\",{})\n",
        "    rel_str = \", \".join([f\"{k}×{c}\" for k,c in rels.items()])\n",
        "    exs = ex_index.get((u,v), [])[:2]\n",
        "    ev_html = \"<br/>\".join([f\"<i>{clean_text(e['predicate'])}</i>: {clean_text(e['sentence'], 240)}\"\n",
        "                            f\"<br/><small>{e.get('source','')} · {e.get('paper_id','')}</small>\"\n",
        "                            for e in exs])\n",
        "    title = f\"<b>{G_viz.nodes[u].get('label',u)}</b> → <b>{G_viz.nodes[v].get('label',v)}</b><br/>\" \\\n",
        "            f\"w={weight}; {rel_str or 'relation'}<br/>{ev_html}\"\n",
        "    net.add_edge(u, v, value=weight, title=title, arrows=\"to\")\n",
        "\n",
        "net.set_options(json.dumps({\n",
        "  \"interaction\": { \"hover\": True, \"tooltipDelay\": 120, \"navigationButtons\": True },\n",
        "  \"physics\": {\n",
        "    \"enabled\": True,\n",
        "    \"barnesHut\": {\"gravitationalConstant\": -8000, \"springLength\": 180, \"springConstant\": 0.015, \"damping\": 0.82},\n",
        "    \"minVelocity\": 0.75\n",
        "  },\n",
        "  \"edges\": {\"smooth\": {\"type\":\"dynamic\"}},\n",
        "  \"layout\": {\"improvedLayout\": True}\n",
        "}))\n",
        "\n",
        "html = net.generate_html()\n",
        "HTML(html)\n",
        "\n",
        "HTML_PATH = str(Path(OUT) / \"kg_pyvis.html\")\n",
        "with open(HTML_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html)\n",
        "print(\"Saved viz →\", HTML_PATH)"
      ],
      "id": "29c223b5-9071-42e5-bd78-f0fb528a4875"
    }
  ]
}