{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVJTe05yUWba54pqiJ1EFd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbanzai88/Book_Writing_Crew/blob/main/Book_writing_Crew_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8VvYqlWZnx7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code doesnt use crewAI or crewai  flow and seems to produce better  results i.e. it has some character development and the number of chapters produced agrees with what was expected."
      ],
      "metadata": {
        "id": "U-2hTvsQdTOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 0) Colab Setup: install & launch Ollama\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "!pip install --quiet langchain-ollama python-docx nest_asyncio tqdm\n",
        "\n",
        "import os, threading, subprocess, time, requests\n",
        "\n",
        "# Unset any OpenAI fallbacks\n",
        "for v in [\"OPENAI_API_KEY\",\"LITELLM_PROVIDER\",\"LITELL M_MODEL\",\"LITELL M_BASE_URL\"]:\n",
        "    os.environ.pop(v, None)\n",
        "\n",
        "os.environ[\"OLLAMA_HOST\"]    = \"127.0.0.1:11434\"\n",
        "os.environ[\"OLLAMA_ORIGINS\"] = \"*\"\n",
        "\n",
        "# Install & start Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh -o install.sh\n",
        "!bash install.sh && chmod +x /usr/local/bin/ollama\n",
        "\n",
        "def _serve_ollama():\n",
        "    subprocess.Popen([\"ollama\",\"serve\"], stderr=subprocess.DEVNULL)\n",
        "threading.Thread(target=_serve_ollama, daemon=True).start()\n",
        "time.sleep(8)\n",
        "print(\"✅ Ollama status:\", requests.get(\"http://127.0.0.1:11434\").status_code)\n",
        "\n",
        "!ollama pull deepseek-r1:1.5b\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 1) Imports & Inputs\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "import json, re, time\n",
        "from typing import List, Any\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import docx\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# User parameters\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "NUM_CH       = 70\n",
        "SEED_IDEA    = (\"Dr. Lena Park — a brilliant but introverted data scientist at Datum, \"\n",
        "                \"a social-media analytics startup. She notices impossible engagement \"\n",
        "                \"patterns in a rising star; her investigation unravels a conspiracy \"\n",
        "                \"of AI-driven “influencers” masquerading as humans—and she must decide \"\n",
        "                \"whether to expose the truth or risk blowing up the platform.\")\n",
        "BOOK_TITLE   = \"Artificial Influencers\"\n",
        "MODE         = \"fiction\"  # or \"philosophy\"\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 2) LLM & Prompt Setup\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "ollama = OllamaLLM(\n",
        "    model=\"deepseek-r1:1.5b\",\n",
        "    base_url=\"http://127.0.0.1:11434\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "outline_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\",\"count\"],\n",
        "    template=(\n",
        "\"You are a creative fiction author. Generate exactly {count} unique chapters \"\n",
        "\"based on:\\n\\n{topic}\\n\\n\"\n",
        "\"Return **only** a JSON array of objects:\\n\"\n",
        "\"```json\\n\"\n",
        "\"[{{\\\"title\\\":\\\"...\\\",\\\"description\\\":\\\"...\\\"}}, ...]\\n\"\n",
        "\"```\"\n",
        "    )\n",
        ")\n",
        "\n",
        "character_prompt = PromptTemplate(\n",
        "    input_variables=[\"outline\",\"num_chars\"],\n",
        "    template=(\n",
        "\"Given this chapter outline:\\n{outline}\\n\\n\"\n",
        "\"Create exactly {num_chars} main characters. Output **only** a JSON array:\\n\"\n",
        "\"```json\\n\"\n",
        "\"[{{\\\"name\\\":\\\"...\\\",\\\"role\\\":\\\"...\\\",\\\"development_arc\\\":\\\"...\\\"}}, ...]\\n\"\n",
        "\"```\"\n",
        "    )\n",
        ")\n",
        "\n",
        "chapter_prompt = PromptTemplate(\n",
        "    input_variables=[\"title\",\"description\",\"idea\"],\n",
        "    template=(\n",
        "\"Write a ~4000-6000 word chapter titled:\\n\\\"{title}\\\"\\n\\n\"\n",
        "\"Seed idea: {idea}\\n\\n\"\n",
        "\"Chapter description:\\n\\\"{description}\\\"\\n\\n\"\n",
        "\"Return text only.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "outline_chain   = LLMChain(prompt=outline_prompt,   llm=ollama)\n",
        "character_chain = LLMChain(prompt=character_prompt, llm=ollama)\n",
        "chapter_chain   = LLMChain(prompt=chapter_prompt,   llm=ollama)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 3) JSON extractor\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "def extract_json_array(raw: Any) -> List[Any]:\n",
        "    txt = raw[\"text\"] if isinstance(raw,dict) else str(raw)\n",
        "    txt = re.sub(r\"<think>.*?</think>\", \"\", txt, flags=re.S)\n",
        "    m = re.search(r\"```json\\s*(\\[[\\s\\S]*?\\])\\s*```\", txt) or re.search(r\"(\\[[\\s\\S]*?\\])\", txt)\n",
        "    if not m:\n",
        "        raise ValueError(\"No JSON array in:\\n\" + txt[:200])\n",
        "    return json.loads(m.group(1))\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 4) Chunked Outline Generation\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "print(f\"→ Generating {NUM_CH}-chapter outline in chunks of 10…\")\n",
        "outline, seen_titles = [], set()\n",
        "to_get = NUM_CH\n",
        "chunk = 10\n",
        "\n",
        "while len(outline) < NUM_CH:\n",
        "    ask = min(chunk, NUM_CH - len(outline))\n",
        "    raw = outline_chain.invoke({\"topic\": SEED_IDEA, \"count\": ask})\n",
        "    try:\n",
        "        partial = extract_json_array(raw)\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Outline chunk failed:\", e)\n",
        "        continue\n",
        "    # dedupe\n",
        "    for ch in partial:\n",
        "        if ch[\"title\"] not in seen_titles and len(outline) < NUM_CH:\n",
        "            outline.append(ch); seen_titles.add(ch[\"title\"])\n",
        "    if not partial:\n",
        "        break\n",
        "\n",
        "print(f\"✔ Final outline: {len(outline)} chapters\\n\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 5) Character Bible\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "NUM_CHAR = max(3, min(10, NUM_CH//8))\n",
        "print(f\"→ Generating {NUM_CHAR} characters…\")\n",
        "raw_chars = character_chain.invoke({\n",
        "    \"outline\": json.dumps(outline),\n",
        "    \"num_chars\": NUM_CHAR\n",
        "})\n",
        "characters = extract_json_array(raw_chars)\n",
        "print(f\"✔ Got {len(characters)} characters\\n\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 6) Chapter Generation\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "print(\"→ Generating chapters in parallel…\")\n",
        "chap_texts = [None]*NUM_CH\n",
        "with ThreadPoolExecutor(max_workers=3) as ex:\n",
        "    futures = {\n",
        "      ex.submit(chapter_chain.invoke, {\n",
        "        \"title\": ch[\"title\"],\n",
        "        \"description\": ch[\"description\"],\n",
        "        \"idea\": SEED_IDEA\n",
        "      }): idx\n",
        "      for idx, ch in enumerate(outline)\n",
        "    }\n",
        "    for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Chapters\"):\n",
        "        idx = futures[fut]\n",
        "        res = fut.result()\n",
        "        chap_texts[idx] = res[\"text\"] if isinstance(res,dict) else str(res)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 7) Save to Word + Download\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "from google.colab import files\n",
        "\n",
        "doc = docx.Document()\n",
        "doc.add_heading(BOOK_TITLE, 0)\n",
        "doc.add_paragraph(f\"Seed idea: {SEED_IDEA}\")\n",
        "\n",
        "# Character Development Section\n",
        "doc.add_page_break()\n",
        "doc.add_heading(\"Character Development\", level=1)\n",
        "for c in characters:\n",
        "    doc.add_heading(c[\"name\"], level=2)\n",
        "    doc.add_paragraph(f\"Role: {c['role']}\")\n",
        "    doc.add_paragraph(c['development_arc'])\n",
        "\n",
        "# Chapters\n",
        "for i, (meta, text) in enumerate(zip(outline, chap_texts), start=1):\n",
        "    doc.add_page_break()\n",
        "    doc.add_heading(f\"Chapter {i}: {meta['title']}\", level=1)\n",
        "    doc.add_paragraph(meta['description'], style=\"Intense Quote\")\n",
        "    doc.add_paragraph(text.strip())\n",
        "\n",
        "fn = BOOK_TITLE.replace(\" \", \"_\") + \".docx\"\n",
        "doc.save(fn)\n",
        "print(f\"📘 Saved {fn}\")\n",
        "files.download(fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "zMtH8VpIbaPG",
        "outputId": "b8cdf3e8-34e9-4b16-c059-3faf21d09a4e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "✅ Ollama status: 200\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "→ Generating 70-chapter outline in chunks of 10…\n",
            "⚠️ Outline chunk failed: No JSON array in:\n",
            "\n",
            "\n",
            "```json\n",
            "[{\n",
            "  \"title\": \"The Disputed AI\",\n",
            "  \"description\": \"Dr. Lena Park investigates an anomaly in the rising star of Datum, where posts are notably less engaging than expected. Her discovery uncov\n",
            "⚠️ Outline chunk failed: Invalid control character at: line 40 column 326 (char 3632)\n",
            "⚠️ Outline chunk failed: Expecting ',' delimiter: line 8 column 5 (char 321)\n",
            "⚠️ Outline chunk failed: Invalid control character at: line 32 column 261 (char 2550)\n",
            "⚠️ Outline chunk failed: Expecting ',' delimiter: line 4 column 5 (char 596)\n",
            "✔ Final outline: 70 chapters\n",
            "\n",
            "→ Generating 8 characters…\n",
            "✔ Got 8 characters\n",
            "\n",
            "→ Generating chapters in parallel…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Chapters: 100%|██████████| 70/70 [10:32:35<00:00, 542.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Saved Artificial_Influencers.docx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_baf677e1-cc0c-458b-87d8-cdd9ce620c79\", \"Artificial_Influencers.docx\", 216583)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}